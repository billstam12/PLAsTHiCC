{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Data\n",
    "\n",
    "The data that was used in this project came from the LSST dataset, an astronomical dataset made by  values based on the survey data of the telescope, that when analyzed will help classify the astronomical data that the telescope will collect in the future.\n",
    "\n",
    "The Data consists of a train set and a test set, containing 7848 and 3490000 stars respectively, making the train to test size ratio rather small. This was the biggest challenge that we faced in this competition, and it was an understandable hurdle regarding the nature of the data. Precision was the goal of this competition and we believe we did a rather good job in that area.\n",
    "\n",
    "##  Equipment Used\n",
    "\n",
    "Due to the huge size of the test set **20GB** we used a google console cloud linux system, equiped with 56GB of RAM and a 10-core processor, to help do the computations as fast as possible. Even so, many ideas we had in mind didn't have the time to be implemented for the competition but we will describe them briefly later.\n",
    "\n",
    "## Data Analysis and Study\n",
    "\n",
    "The data was divided into two sets. One for the **timeseries data** of each star, that contained the *Julian Dates, the flux and the corresponding passband* of each observation, while also having the *flux error* of the observation and a *detected* value that was equal to 1 if the object's brightness is significantly different at the 3-sigma level relative to the reference template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>mjd</th>\n",
       "      <th>passband</th>\n",
       "      <th>flux</th>\n",
       "      <th>flux_err</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4229</td>\n",
       "      <td>2</td>\n",
       "      <td>-544.810303</td>\n",
       "      <td>3.622952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4306</td>\n",
       "      <td>1</td>\n",
       "      <td>-816.434326</td>\n",
       "      <td>5.553370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4383</td>\n",
       "      <td>3</td>\n",
       "      <td>-471.385529</td>\n",
       "      <td>3.801213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4450</td>\n",
       "      <td>4</td>\n",
       "      <td>-388.984985</td>\n",
       "      <td>11.395031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>615</td>\n",
       "      <td>59752.4070</td>\n",
       "      <td>2</td>\n",
       "      <td>-681.858887</td>\n",
       "      <td>4.041204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id         mjd  passband        flux   flux_err  detected\n",
       "0        615  59750.4229         2 -544.810303   3.622952         1\n",
       "1        615  59750.4306         1 -816.434326   5.553370         1\n",
       "2        615  59750.4383         3 -471.385529   3.801213         1\n",
       "3        615  59750.4450         4 -388.984985  11.395031         1\n",
       "4        615  59752.4070         2 -681.858887   4.041204         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../all/training_set.csv\")\n",
    "df_meta = pd.read_csv(\"../all/training_set_metadata.csv\").set_index(\"object_id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other data file contains the metadata of each star, differentiating the stars by an object_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ra</th>\n",
       "      <th>decl</th>\n",
       "      <th>gal_l</th>\n",
       "      <th>gal_b</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hostgal_specz</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>distmod</th>\n",
       "      <th>mwebv</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>349.046051</td>\n",
       "      <td>-61.943836</td>\n",
       "      <td>320.796530</td>\n",
       "      <td>-51.753706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>53.085938</td>\n",
       "      <td>-27.784405</td>\n",
       "      <td>223.525509</td>\n",
       "      <td>-54.460748</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8181</td>\n",
       "      <td>1.6267</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>45.4063</td>\n",
       "      <td>0.007</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>33.574219</td>\n",
       "      <td>-6.579593</td>\n",
       "      <td>170.455585</td>\n",
       "      <td>-61.548219</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>40.2561</td>\n",
       "      <td>0.021</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0.189873</td>\n",
       "      <td>-45.586655</td>\n",
       "      <td>328.254458</td>\n",
       "      <td>-68.969298</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3037</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>1.1523</td>\n",
       "      <td>40.7951</td>\n",
       "      <td>0.007</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>352.711273</td>\n",
       "      <td>-63.823658</td>\n",
       "      <td>316.922299</td>\n",
       "      <td>-51.059403</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>40.4166</td>\n",
       "      <td>0.024</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ra       decl       gal_l      gal_b  ddf  hostgal_specz  \\\n",
       "object_id                                                                     \n",
       "615        349.046051 -61.943836  320.796530 -51.753706    1         0.0000   \n",
       "713         53.085938 -27.784405  223.525509 -54.460748    1         1.8181   \n",
       "730         33.574219  -6.579593  170.455585 -61.548219    1         0.2320   \n",
       "745          0.189873 -45.586655  328.254458 -68.969298    1         0.3037   \n",
       "1124       352.711273 -63.823658  316.922299 -51.059403    1         0.1934   \n",
       "\n",
       "           hostgal_photoz  hostgal_photoz_err  distmod  mwebv  target  \n",
       "object_id                                                              \n",
       "615                0.0000              0.0000      NaN  0.017      92  \n",
       "713                1.6267              0.2552  45.4063  0.007      88  \n",
       "730                0.2262              0.0157  40.2561  0.021      42  \n",
       "745                0.2813              1.1523  40.7951  0.007      90  \n",
       "1124               0.2415              0.0176  40.4166  0.024      90  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Distribution\n",
    "\n",
    "The goal of the challenge is to classify each star to a target, based on features we will extract from the above data. Even though a simple classification problem sounds easy, the fact that the data ratio is skewed is not the only problem that prevents the problem from being simple. Looking at the graph below ***Taken from Study.ipynb***, we can see that the distribution of the targets are not equal.\n",
    "\n",
    "<img src=\"img/targets.png\">\n",
    "\n",
    "For this reason, when classifying the Data we preproccessed it with **SMOTE**, which is a certain algorithm for providing the training set with more data, as to stabilize the disparities between the unequal size classes.\n",
    "\n",
    "*For more on SMOTE you can visit this link  https://arxiv.org/pdf/1106.1813.pdf*\n",
    "\n",
    "## Viewing the data and inferring our next move\n",
    "### Autoencoded time-series data\n",
    "\n",
    "By plotting a random flux \n",
    "\n",
    "<img src = \"img/flux_example.png\">\n",
    "\n",
    "We can see that it is divided in 3 to 4 different time ranges, making the study of such a timeseries harder than normal as it is missing values in certain places. To compat this, we used an Autoencoder later in the challenge, that was train on the data and made it's own normalized and time invariant data. This could help in 2 ways\n",
    "1) Extract Random Features that the autoencoder found useful\n",
    "2) Use the encoded model's data to re-train the model\n",
    "\n",
    "The first helped our results quite a lot, while the second was one of those things that we didn't have the time to finish, as the autoencoded model just for the training set upped its size by 200%. (And that's because it was filling gaps in the data). This means that the test set variant would be quite huge.\n",
    "\n",
    "The code for both of these can be found in *python_files_test/autoenc.py, python_files_test/encoder.py*\n",
    "\n",
    "### Dividing the flux\n",
    "\n",
    "Another way we tried to compat the above was, by dividing the flux series in 3-4 pieces based on a threshold and then applying feature extraction in these places seperately. This worked great in the training set, but failed to work in the test set cause we didn't took into account that the data there was seperated a lot differently. The code for the above is given below:<br>\n",
    "\n",
    "```python\n",
    "\n",
    "def sxedio5(series):\n",
    "    threshold = 120 #Threshold found as the minimum of all the \"dead-time-spaces\"\n",
    "    diffs = np.diff(series.mjd)\n",
    "    diff_ids = (np.where(diffs>threshold))\n",
    "    print(diff_ids)\n",
    "    \n",
    "    if(len(diff_ids[0])==2):\n",
    "        series1 = series[:diff_ids[0][0]]\n",
    "        series2 = series[diff_ids[0][0]:diff_ids[0][1]]\n",
    "        series3 = series[diff_ids[0][1]:]\n",
    "        agg1 = series1.agg(aggs_all)\n",
    "        agg2 = series2.agg(aggs_all)\n",
    "        agg3 = series3.agg(aggs_all)\n",
    "        aggs = pd.concat([agg1, agg2, agg3], axis=1)\n",
    "        \n",
    "    elif(len(diff_ids[0])==3):\n",
    "        series1 = series[:diff_ids[0][0]]\n",
    "        series2 = series[diff_ids[0][0]:diff_ids[0][1]]\n",
    "        series3 = series[diff_ids[0][1]:diff_ids[0][2]]\n",
    "        series4 = series[diff_ids[0][2]:]\n",
    "        agg1 = series1.agg(aggs_all)\n",
    "        agg2 = series2.agg(aggs_all)\n",
    "        agg3 = series3.agg(aggs_all)\n",
    "        agg4 = series4.agg(aggs_all)\n",
    "        aggs = pd.concat([agg1, agg2, agg3, agg4], axis=1)\n",
    "        \n",
    "    else:\n",
    "        mjd_diff = np.max(series.mjd) - np.min(series.mjd)\n",
    "        mjd_diff_div =int(np.ceil(mjd_diff/4))\n",
    "        #print(mjd_diff, mjd_diff_div)\n",
    "        series1 = series[:mjd_diff_div]\n",
    "        series2 = series[mjd_diff_div:2*mjd_diff_div]\n",
    "        series3 = series[2*mjd_diff_div:3*mjd_diff_div]\n",
    "        series4 = series[3*mjd_diff_div:]\n",
    "        agg1 = series1.agg(aggs_all)\n",
    "        agg2 = series2.agg(aggs_all)\n",
    "        agg3 = series3.agg(aggs_all)\n",
    "        aggs = pd.concat([agg1, agg2, agg3], axis=1)\n",
    "        \n",
    "    if(aggs.shape == (6,9)):\n",
    "        agg1[:] = 0\n",
    "        #print(agg1)\n",
    "        aggs = pd.concat([aggs,agg1],axis=1)\n",
    "    return aggs   \n",
    "```\n",
    "\n",
    "Because the above failed to work in the test-set, we just did statistical analysis to the set without any further preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we Tackled the problem\n",
    "\n",
    "In *Study.ipynb* we plotted 20 fluxes for each target class to see how they compare, and by that analysis we got most of the ideas on how to tackle the classification problem.\n",
    "\n",
    "<img src=\"img/flux_target.png\">\n",
    "\n",
    "We can see that same-class objects look a lot like each other, as they have the **same peak-patterns** the **same passband-patterns** and sometimes **same times of observations.**\n",
    "\n",
    "For that reason, we extracted the following statistical features for each object_id, by sorting for each different passband and by not.<br>\n",
    "*Most of the work for Feature Extraction, can be found in the kernel Feature Extraction_neg.ipynb and in the file agg_test.py, the code is given below*\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def process_flux(df):\n",
    "    # Get the squared ratio of the flux and flux_err as a feature\n",
    "    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n",
    "\n",
    "    df_flux = pd.DataFrame({\n",
    "        'flux_ratio_sq': flux_ratio_sq, \n",
    "        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,}, \n",
    "        index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux], axis=1)\n",
    "\n",
    "\n",
    "def process_flux_agg(df):\n",
    "    # Create more flux features by the statistical deviations between the flux, its error and the squared\n",
    "    # error from before\n",
    "    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n",
    "    flux_diff = df['flux_max'].values - df['flux_min'].values\n",
    "    \n",
    "    df_flux_agg = pd.DataFrame({\n",
    "        'flux_w_mean': flux_w_mean,\n",
    "        'flux_diff1': flux_diff,\n",
    "        'flux_diff2': flux_diff / df['flux_mean'].values,       \n",
    "        'flux_diff3': flux_diff /flux_w_mean,\n",
    "        }, index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux_agg], axis=1)\n",
    "\n",
    "def process_meta(filename):\n",
    "    meta_df = pd.read_csv(filename)\n",
    "    \n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df\n",
    "\n",
    "# General aggregates\n",
    "aggs_all = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq':['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "    }\n",
    "\n",
    "aggs_pb = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_ratio_sq':['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "    }\n",
    "\n",
    "# Aggregates for tsfresh. Tsfresh is a python modeling from timeseries feature extraction!\n",
    "fcp = {\n",
    "        'flux': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "            'length': None,\n",
    "        },\n",
    "                \n",
    "        'flux_by_flux_ratio_sq': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,       \n",
    "        },\n",
    "                \n",
    "        'flux_passband': {\n",
    "            'fft_coefficient': [\n",
    "                    {'coeff': 0, 'attr': 'abs'}, \n",
    "                    {'coeff': 1, 'attr': 'abs'}\n",
    "                ],\n",
    "            'kurtosis' : None, \n",
    "            'skewness' : None,\n",
    "        },\n",
    "                \n",
    "        'mjd': {\n",
    "            'maximum': None, \n",
    "            'minimum': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "        },\n",
    "    }\n",
    "```\n",
    "\n",
    "## Our own features\n",
    "\n",
    "In most of the files we have implemented our own algorithms for extracting features, which are the following.<br>\n",
    "1) Find the number of local maximas in a flux curve.<br>\n",
    "2) Find the Dynamic Time Wrapping (DTW) between each passband for one curve (meaning 15 features for all of 6 passbands).<br>\n",
    "3) Find the first time changes for each curve (derivation).<br>\n",
    "4) Find the longest time that the flux keeps falling, and calculate the peak time interval.<br>\n",
    "\n",
    "### Metadata features<br>\n",
    "In the data we have two kinds of values that help find the host galaxy of the star, and these are **hostgal_specz** and (**hostgal_photoz, hostgal_photoz_err**). The first of these values, is of greater importance and could help our work greatly, but it is not available in the whole of the test set. For this, we tried to fit the hostgal_photoz of each star and it's distmod to the hostgal_specz, and obtained the following equation that we used to calculate this new feature: <br>\n",
    "\n",
    "```python     \n",
    "meta_df[\"hostgal_photoz_certain_mine\"] = -6.09*(10**(-14))*np.exp(0.6713*meta_df[\"distmod\"])+0.04902+meta_df[\"hostgal_photoz\"]\n",
    "```\n",
    "\n",
    "For what concerns the distances of each star, we didn't do much as they are universal for all classes \n",
    "<img src=\"img/position_example.png\">\n",
    "\n",
    "Even so we used the following haversine function to compare the galactic and earthly distances of the two for any diviations\n",
    "\n",
    "```python\n",
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }\n",
    "```\n",
    "\n",
    "## Other hurdles and things that could have been implemented\n",
    "\n",
    "Another problem with the data, that didn't help with the classification was that *target 99* was absent from the test data, for this reason we had to calculate the probability of one object being there based on the weighted probabilities of the other classes, and for that reason we used the following function.<br>\n",
    "\n",
    "```python\n",
    "# Get the median, mean and max of the other probabilities and infer the  probability of class 99 based on these\n",
    "def GenUnknown(data):\n",
    "    return ((((((data[\"mymedian\"]) + (((data[\"mymean\"]) / 2.0)))/2.0)) + (((((1.0) - (((data[\"mymax\"]) * (((data[\"mymax\"]) * (data[\"mymax\"]))))))) / 2.0)))/2.0)\n",
    "\n",
    "feats = ['class_6', 'class_15', 'class_16', 'class_42', 'class_52', 'class_53',\n",
    "         'class_62', 'class_64', 'class_65', 'class_67', 'class_88', 'class_90',\n",
    "         'class_92', 'class_95']\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y['mymean'] = preds_df[feats].mean(axis=1)\n",
    "y['mymedian'] = preds_df[feats].median(axis=1)\n",
    "y['mymax'] = preds_df[feats].max(axis=1)\n",
    "\n",
    "class_99 = GenUnknown(y)\n",
    "preds_df[\"class_99\"] = class_99\n",
    "```\n",
    "\n",
    "### Reading the test set\n",
    "For doing this we used a library created by an other competitor, that can be found in the read_test_files folder\n",
    "\n",
    "### Gaussian Process\n",
    "Maybe the only thing that we didn't have time to implement and could improve our score for the best was to fit Gaussian Processes to the time-series data, so we could infer statistical features for every different set of observations. A gaussian process is a set of functions that can fit data without using any parameters. This means that we can feed any curve in a good algorithm and it will return the posterior parameters best suited for the fit.\n",
    "<br>\n",
    "*For more on GP visit this link http://katbailey.github.io/post/gaussian-processes-for-dummies/*\n",
    "<br>\n",
    "\n",
    "Generally this is a time consuming problem, and for that we have to constraint the function to only fit processes of certain range, curvature, noise and other such characteristics.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "In the file **Deep Learning.ipynb** we used a simple LGB Random Forest to make predictions on the data.<br>\n",
    "The reason that we used it besides it generally giving the best predictions from all other models (RNN, LSTMNN, Machine Learning tasks), was because of its speed. The speed in this competition was key, and by using this algorithm that could train in less than a minute for 10-fold CV we managed to save a lot of much needed time, for our other calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
